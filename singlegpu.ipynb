{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolmou/linguas-indigenas/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from multigpu import main\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL)\n",
    "MAX_LEN = 128\n",
    "CLS = 101\n",
    "SEP = 102\n",
    "MASKED = 103\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 3\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide dataset and masking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = {\n",
    "    'carolina': 0,\n",
    "    'psa_small': 0, # portuguese sentiment analysis small\n",
    "    'psa_full': 0 # portuguese sentiment analysis full\n",
    "}\n",
    "available_masking_strategies = {\n",
    "    'end_token': 0,\n",
    "    'random_tokens': 0\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = 'carolina'\n",
    "chosen_masking_strategy = 'end_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=f'{chosen_dataset}/{chosen_masking_strategy}/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carolina_dataset():\n",
    "    \"\"\"\n",
    "    Returns a dictionary {'train', 'test'}, where\n",
    "    each one of the keys is a list of  strings.\n",
    "    \"\"\"\n",
    "    corpus_carolina = load_dataset(\"carolina-c4ai/corpus-carolina\", taxonomy=\"wik\", revision=\"v1.2\")\n",
    "    sentences_carolina = corpus_carolina['corpus']['text']\n",
    "    sep_carolina = int(0.7*len(sentences_carolina))\n",
    "    train = sentences_carolina[:sep_carolina]\n",
    "    test = sentences_carolina[sep_carolina:]\n",
    "\n",
    "    return {\n",
    "        'train': train,\n",
    "        'test': test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psa_full_dataset():\n",
    "    \"\"\"\n",
    "    Returns a dictionary {'train', 'test'}, where\n",
    "    each one of the keys is a list of  strings.\n",
    "    \"\"\"\n",
    "    portuguese_sentiment_analysis = load_dataset('jvanz/portuguese_sentiment_analysis')\n",
    "    train = portuguese_sentiment_analysis['train']['review_text_processed']\n",
    "    test = portuguese_sentiment_analysis['test']['review_text_processed']\n",
    "    return {\n",
    "        'train': train,\n",
    "        'test': test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psa_small_dataset():\n",
    "    path='../../data/portuguese_sentiment_analysis/'\n",
    "    return_dic = {}\n",
    "    \n",
    "    for type in ['train', 'test']:\n",
    "        data = load_from_disk(path+type)\n",
    "        return_dic[type] = data['review_text_processed']\n",
    "\n",
    "    return return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets['carolina'] = carolina_dataset\n",
    "available_datasets['psa_full'] = psa_full_dataset\n",
    "available_datasets['psa_small'] = psa_small_dataset "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_CLS_and_SEP_false(id_tensor, true_false_tensor):\n",
    "    \"\"\"\n",
    "    Receives a tensor of ids and a tensor of masking, with\n",
    "    True and False values. Returns a copy of the true_false_tensor\n",
    "    where every CLS and SEP position is for sure set to False.\n",
    "\n",
    "    Doesn't mutate any input value and it's free of aliasing.\n",
    "    \"\"\"\n",
    "    ans = true_false_tensor.detach().clone()\n",
    "    ans = ans & (id_tensor != CLS) & (id_tensor != SEP)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_random_positions(tensor, k = 0.15):\n",
    "    \"\"\"\n",
    "    Receives a tensor and returns\n",
    "    a new tensor of the same shape randomly filled\n",
    "    with True's at the rate k.\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    rand = torch.rand(shape)\n",
    "    return (rand < k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_end_positions(tensor_ids):\n",
    "  \"\"\"\n",
    "  Receives a tensor and returns\n",
    "  a tensor of the same shape filled with False's\n",
    "  and a single True, at the end.\n",
    "  \"\"\"\n",
    "  shape = tensor_ids.shape\n",
    "  masks = torch.zeros(shape) != 0\n",
    "\n",
    "  for i in range(0, shape[0]):\n",
    "    for j in range(0, masks[i].shape[0]):\n",
    "      if tensor_ids[i][j] == SEP:\n",
    "        masks[i][j-1] = True\n",
    "  return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_nothing(tensor):\n",
    "    \"\"\"\n",
    "    Receives a tensor and returns a tensor\n",
    "    of the same shape filled with False's.\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    return torch.full(shape, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(masking_strategy, tensor):\n",
    "    shape = tensor.shape\n",
    "    mask_arr = masking_strategy(tensor)\n",
    "    mask_arr = assure_CLS_and_SEP_false(tensor, mask_arr)\n",
    "    rows = shape[0]\n",
    "\n",
    "    def indices_to_mask(idx):\n",
    "        \"\"\"\n",
    "        Returns a list containing all positions in the\n",
    "        idx-th row of mask_arr that have to be masked.\n",
    "\n",
    "        0 <= idx < rows has to be satisfied.\n",
    "        \"\"\"\n",
    "        to_mask_positions = mask_arr[idx].nonzero()\n",
    "        return torch.flatten(to_mask_positions).tolist()\n",
    "\n",
    "    for i in range(rows):\n",
    "        selection = indices_to_mask(i)\n",
    "        tensor[i, selection] = MASKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels_and_ids(sentences, path, masking_strategy):\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "    \n",
    "    inputs = tokenizer(sentences, return_tensors = 'pt', max_length = MAX_LEN, \\\n",
    "                        padding = True, truncation = True)\n",
    "        \n",
    "    inputs['labels'] = inputs['input_ids'].detach().clone()\n",
    "\n",
    "    mask(masking_strategy, inputs['input_ids'])\n",
    "\n",
    "    torch.save({\n",
    "        'labels': inputs['labels'],\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }, f'{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_masking_strategies['end_token'] = mask_end_positions \n",
    "available_masking_strategies['random_tokens'] = mask_random_positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_device(arg):\n",
    "    return arg.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nget_dataset_function = available_datasets[chosen_dataset]\\nget_masking_strategy_function = available_masking_strategies[chosen_masking_strategy]\\n\\ndataset = get_dataset_function()\\n\\ntrain = dataset['train']\\ntest = dataset['test']\\n\\n# for type, sentences in [('train', train), ('test', test)]:\\n#     print(f'{type}:')\\n#     print(f'# sentences: {len(sentences)}')\\n#     print(f'small sample: {sentences[:3]}\\n')\\n\\n#     save_labels_and_ids(sentences, f'{chosen_dataset}/{chosen_masking_strategy}/{type}_tokens', #                         get_masking_strategy_function)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "get_dataset_function = available_datasets[chosen_dataset]\n",
    "get_masking_strategy_function = available_masking_strategies[chosen_masking_strategy]\n",
    "\n",
    "dataset = get_dataset_function()\n",
    "\n",
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "\n",
    "# for type, sentences in [('train', train), ('test', test)]:\n",
    "#     print(f'{type}:')\n",
    "#     print(f'# sentences: {len(sentences)}')\n",
    "#     print(f'small sample: {sentences[:3]}\\n')\n",
    "\n",
    "#     save_labels_and_ids(sentences, f'{chosen_dataset}/{chosen_masking_strategy}/{type}_tokens', \\\n",
    "#                         get_masking_strategy_function)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(path, params):\n",
    "\n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, input):\n",
    "            self.input = input\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return { key: torch.tensor(val[idx]) for key, val in self.input.items() }\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input['input_ids'])\n",
    "\n",
    "    \"\"\"\n",
    "    # uncomment the following lines if you don't want\n",
    "    # to sanity check\n",
    "    amount_to_print = 5\n",
    "    print(f\"printing first {amount_to_print}:\") \n",
    "    for id, label in zip(inputs['input_ids'][:amount_to_print], inputs['labels'][:amount_to_print]):\n",
    "        print(f\"id: {''.join(tokenizer.decode(id))}\")\n",
    "        print(f\"label: {''.join(tokenizer.decode(label))}\")\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = torch.load(path) \n",
    "    return torch.utils.data.DataLoader( MyDataset(inputs), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_strategy = available_masking_strategies[chosen_masking_strategy]\n",
    "train_dataloader = create_dataloader(path+'train_tokens', train_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement early stopping, that will print the best model if the next \"early_stopping_patience\" models are worse than it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 100 # best loss that the current best model yielded\n",
    "best_model_state = -1 # the best model so far\n",
    "early_stopping_patience = 2 # number of models we have to check to decide a best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = send_to_device(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(num_epoch=0):\n",
    "    \"\"\" \n",
    "    Returns the loss yielded by the model's current state.\n",
    "    Modifies the state of the model, leaving the state\n",
    "    that yields the returned loss.\n",
    "\n",
    "    Receives a num_epoch that will be printed in the progress\n",
    "    feedback.\n",
    "    \"\"\"\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for batch in loop:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        labels =batch['labels']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        input_ids = send_to_device(input_ids)\n",
    "        labels = send_to_device(labels)\n",
    "        attention_mask = send_to_device(attention_mask)\n",
    "\n",
    "        output = model(input_ids, labels=labels, attention_mask = attention_mask)\n",
    "        \n",
    "        loss = output.loss\n",
    "        average_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "       \n",
    "        loop.set_description(f'Epoch {num_epoch}')\n",
    "        loop.set_postfix(loss = loss.item())\n",
    "\n",
    "    qtd = len(train_dataloader)\n",
    "\n",
    "    return average_loss/qtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21004 [00:00<?, ?it/s]/tmp/ipykernel_2448311/3729909520.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return { key: torch.tensor(val[idx]) for key, val in self.input.items() }\n",
      "Epoch 1:   0%|          | 5/21004 [00:02<2:21:02,  2.48it/s, loss=1.28] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m early_patience_counter \u001b[39m<\u001b[39m early_stopping_patience:\n\u001b[1;32m      5\u001b[0m     run_epochs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     loss \u001b[39m=\u001b[39m run_epoch(run_epochs)\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_loss:\n\u001b[1;32m     10\u001b[0m         best_loss \u001b[39m=\u001b[39m loss \n",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(num_epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     27\u001b[0m average_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 28\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     29\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m loop\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mnum_epoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/linguas-indigenas/.venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/linguas-indigenas/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_patience_counter = 0\n",
    "run_epochs = 0\n",
    "\n",
    "while early_patience_counter < early_stopping_patience:\n",
    "    run_epochs += 1\n",
    "    \n",
    "    loss = run_epoch(run_epochs)\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss \n",
    "        best_model = model.state_dict()\n",
    "        early_patience_counter = 0\n",
    "        torch.save(best_model, path+f'model_at_epoch_{run_epochs}')\n",
    "    else:\n",
    "        early_patience_counter += 1\n",
    "\n",
    "    print(f'Current loss is {loss}, and best loss is {best_loss}')\n",
    "\n",
    "#torch.save(best_model, path+'best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, path+'best_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current biggest prob eh 0.5018615126609802 com palavra .\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " e has prob 0.00496727554127574\n",
      "\n",
      "Found a new completion Hoje eu vou comer\n",
      "Current biggest prob eh 0.00496727554127574 com palavra  e\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " de has prob 0.004598874598741531\n",
      "\n",
      "Current biggest prob eh 0.004598874598741531 com palavra  de\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " comer has prob 0.0037914386484771967\n",
      " e cantar has prob 0.0003103567582770604\n",
      "\n",
      "Current biggest prob eh 0.0037914386484771967 com palavra  comer\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      "o has prob 0.0034122997894883156\n",
      " de lá has prob 0.00018149149761011174\n",
      " e cantar has prob 0.0003103567582770604\n",
      "\n",
      "Current biggest prob eh 0.0034122997894883156 com palavra o\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " com has prob 0.0032536862418055534\n",
      " comer. has prob 0.0018488998292404482\n",
      " e cantar has prob 0.0003103567582770604\n",
      " de lá has prob 0.00018149149761011174\n",
      "\n",
      "Current biggest prob eh 0.0032536862418055534 com palavra  com\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " sob has prob 0.0029481847304850817\n",
      "o. has prob 0.0019943316509858366\n",
      " e cantar has prob 0.0003103567582770604\n",
      " de lá has prob 0.00018149149761011174\n",
      " comer. has prob 0.0018488998292404482\n",
      "\n",
      "Current biggest prob eh 0.0029481847304850817 com palavra  sob\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " mais has prob 0.0023201724980026484\n",
      " comer. has prob 0.0018488998292404482\n",
      "o. has prob 0.0019943316509858366\n",
      " de lá has prob 0.00018149149761011174\n",
      " e cantar has prob 0.0003103567582770604\n",
      " com ela has prob 0.0005192582306291216\n",
      "\n",
      "Current biggest prob eh 0.0023201724980026484 com palavra  mais\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      "er has prob 0.002042921259999275\n",
      " comer. has prob 0.0018488998292404482\n",
      "o. has prob 0.0019943316509858366\n",
      " de lá has prob 0.00018149149761011174\n",
      " e cantar has prob 0.0003103567582770604\n",
      " com ela has prob 0.0005192582306291216\n",
      " sob. has prob 0.0009836959052681449\n",
      "\n",
      "Current biggest prob eh 0.002042921259999275 com palavra er\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      "o. has prob 0.0019943316509858366\n",
      " a has prob 0.0019516784232109785\n",
      " sob. has prob 0.0009836959052681449\n",
      " comer. has prob 0.0018488998292404482\n",
      " e cantar has prob 0.0003103567582770604\n",
      " com ela has prob 0.0005192582306291216\n",
      " de lá has prob 0.00018149149761011174\n",
      " mais. has prob 0.001230435235917357\n",
      "\n",
      "Current biggest prob eh 0.0019943316509858366 com palavra o.\n",
      "Size of the heap None\n",
      "Printing heap:\n",
      " a has prob 0.0019516784232109785\n",
      " comer. has prob 0.0018488998292404482\n",
      " sob. has prob 0.0009836959052681449\n",
      "er. has prob 0.0013481725879571638\n",
      " e cantar has prob 0.0003103567582770604\n",
      " com ela has prob 0.0005192582306291216\n",
      " de lá has prob 0.00018149149761011174\n",
      " mais. has prob 0.001230435235917357\n",
      "oo has prob 4.61379495130497e-06\n",
      "\n",
      "Found a new completion Hoje eu vou comero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from next_sentence_prediction.masked_llm.next_word_algorithms.priority_queue.predictions import get_all_predictions\n",
    "model.to('cpu')\n",
    "get_all_predictions('Hoje eu vou comer', tokenizer, model, 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ponctuation(sentence):\n",
    "  ponctuation_array = ['.', '!', '?', ';', '...']\n",
    "\n",
    "  chars = [char for char in sentence]\n",
    "\n",
    "  while chars and chars[-1] in ponctuation_array:\n",
    "    chars = chars[:-1]\n",
    "\n",
    "  sentence = ''.join(chars)\n",
    "\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences_no_points = [ remove_ponctuation(sentence) for sentence in raw_sentences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_word(sentence):\n",
    "  \"\"\"\n",
    "  Receives a sentence and return a tuple where the first\n",
    "  element is the sentence without the last word and the\n",
    "  second element is the last word itself.\n",
    "  \"\"\"\n",
    "  words = sentence.split()\n",
    "  return \" \".join(words[:-1]), words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masked_token(sentence):\n",
    "    return sentence + \" <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for sentence in raw_sentences_no_points if sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_acc = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 87/14913 [00:00<02:20, 105.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14913/14913 [02:25<00:00, 102.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# for visuals\n",
    "loop = tqdm(sentences, leave=True)\n",
    "\n",
    "processed_up_to_now = 0\n",
    "\n",
    "accuracies = [0] * desired_acc\n",
    "\n",
    "for sentence in loop:\n",
    "\n",
    "    last_word_removed, last_word = remove_last_word(sentence)\n",
    "    last_word_removed = add_masked_token(last_word_removed)\n",
    "\n",
    "    try:\n",
    "        bert = get_all_predictions(last_word_removed, desired_acc)\n",
    "    except:\n",
    "        # if i fell here, the sentence reached the maximum amount\n",
    "        # of tokens, so we'll skip it\n",
    "        continue\n",
    "    if last_word in bert:\n",
    "        ix = bert.index(last_word)\n",
    "        accuracies[ix] += 1\n",
    "\n",
    "    processed_up_to_now += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path+'accuracy.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 13.696047404215204%\n",
      "2: 17.487037909905055%\n",
      "3: 19.37243283280587%\n",
      "4: 21.129890243081274%\n",
      "5: 22.33519628307858%\n",
      "6: 23.304827957713286%\n",
      "7: 24.02531816039324%\n",
      "8: 24.685206383408527%\n",
      "9: 25.32489394653559%\n",
      "10: 25.850111103629388%\n",
      "11: 26.32819338765066%\n",
      "12: 26.765874351895498%\n",
      "13: 27.041950037034546%\n",
      "14: 27.318025722173587%\n",
      "15: 27.540232980944047%\n"
     ]
    }
   ],
   "source": [
    "up_to_now = 0\n",
    "for top, acc_val in enumerate(accuracies):\n",
    "    up_to_now += acc_val \n",
    "    percentage = (up_to_now/processed_up_to_now)*100\n",
    "    str = f\"{top+1}: {percentage}%\"\n",
    "    file.write(str+'\\n')\n",
    "    print(str)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
