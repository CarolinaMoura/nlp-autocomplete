{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "from dijkstra.predictions import create_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = '../../data/test.csv'\n",
    "model_checkpoints = []\n",
    "\n",
    "datasets = ['carolina', 'psa_small', 'psa_full']\n",
    "models = ['end_token', 'random_tokens']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        model_checkpoints.append(f'../../models/{dataset}/{model}/best_model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model.load_state_dict(torch.load('../../models/carolina/random_tokens/best_model.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>last_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Colossal é um filme de comédia, ação-thriller ...</td>\n",
       "      <td>Vigalondo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O filme é protagonizado por Anne Hathaway, Dan...</td>\n",
       "      <td>Nelson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O filme teve estreia no Festival Internacional...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Está programado para ser lançado pela NEON em ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enredo Depois de perder seu emprego e namorado...</td>\n",
       "      <td>Seul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47021</th>\n",
       "      <td>Nosso compromisso, por meio do Ibross, é contr...</td>\n",
       "      <td>país</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47022</th>\n",
       "      <td>RENILSON REHEM DE SOUZA, médico, é presidente ...</td>\n",
       "      <td>Saúde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47023</th>\n",
       "      <td>br Os artigos publicados com assinatura não tr...</td>\n",
       "      <td>jornal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47024</th>\n",
       "      <td>Sua publicação obedece ao propósito de estimul...</td>\n",
       "      <td>contemporâneo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47025</th>\n",
       "      <td>@vaaaifelipe Se preferir nos procure através d...</td>\n",
       "      <td>cancelarem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47026 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence      last_word\n",
       "0      Colossal é um filme de comédia, ação-thriller ...      Vigalondo\n",
       "1      O filme é protagonizado por Anne Hathaway, Dan...         Nelson\n",
       "2      O filme teve estreia no Festival Internacional...           2016\n",
       "3      Está programado para ser lançado pela NEON em ...           2017\n",
       "4      Enredo Depois de perder seu emprego e namorado...           Seul\n",
       "...                                                  ...            ...\n",
       "47021  Nosso compromisso, por meio do Ibross, é contr...           país\n",
       "47022  RENILSON REHEM DE SOUZA, médico, é presidente ...          Saúde\n",
       "47023  br Os artigos publicados com assinatura não tr...         jornal\n",
       "47024  Sua publicação obedece ao propósito de estimul...  contemporâneo\n",
       "47025  @vaaaifelipe Se preferir nos procure através d...     cancelarem\n",
       "\n",
       "[47026 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_predictions = create_functions(tokenizer, model, 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sentences = df['sentence']\n",
    "random.shuffle(sentences)\n",
    "sentences = sentences[:20]\n",
    "last_words = ['temporada']\n",
    "corrects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:45.847408\n",
      "10.8\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "top_k = 5\n",
    "average_calls = 0\n",
    "\n",
    "for s in sentences:\n",
    "    average_calls += get_all_predictions(s,top_k, False,)[-1]\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print(end_time-start_time)\n",
    "print(average_calls/len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_model(model_name):\n",
    "    corrects[model_name] = [0] * top_k\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    loop = tqdm(zip(sentences, last_words), total=total_samples, leave=True)\n",
    "\n",
    "    for sent, word in loop:\n",
    "        suggestions = get_all_predictions(sent, tokenizer, model, top_k, False, 'cuda:1')\n",
    "\n",
    "        try:\n",
    "            # gotta add this extra space because\n",
    "            # the prediction of a new word also\n",
    "            # predicts a preceding whitespace\n",
    "            ix = suggestions.index(' '+word)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        corrects[model_name][ix] += 1\n",
    "\n",
    "    tot = 0\n",
    "\n",
    "    for ix, val in enumerate(corrects[model_name]):\n",
    "        tot += val\n",
    "        print(f'Top {ix+1}: {tot}/{total_samples} = {tot/total_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ../../models/carolina/end_token/best_model.pt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0/1 = 0.0\n",
      "Top 2: 0/1 = 0.0\n",
      "Top 3: 0/1 = 0.0\n",
      "Top 4: 0/1 = 0.0\n",
      "Top 5: 1/1 = 1.0\n",
      "Running ../../models/carolina/random_tokens/best_model.pt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0/1 = 0.0\n",
      "Top 2: 0/1 = 0.0\n",
      "Top 3: 0/1 = 0.0\n",
      "Top 4: 1/1 = 1.0\n",
      "Top 5: 1/1 = 1.0\n",
      "Running ../../models/psa_small/end_token/best_model.pt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m model_checkpoint \u001b[39min\u001b[39;00m model_checkpoints:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRunning \u001b[39m\u001b[39m{\u001b[39;00mmodel_checkpoint\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     run_for_model(model_checkpoint)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mrun_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m loop \u001b[39m=\u001b[39m tqdm(\u001b[39mzip\u001b[39m(sentences, last_words), total\u001b[39m=\u001b[39mtotal_samples, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m sent, word \u001b[39min\u001b[39;00m loop:\n\u001b[0;32m----> 7\u001b[0m     suggestions \u001b[39m=\u001b[39m get_all_predictions(sent, tokenizer, model, top_k, \u001b[39mFalse\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39mcuda:1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[39m# gotta add this extra space because\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[39m# the prediction of a new word also\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[39m# predicts a preceding whitespace\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         ix \u001b[39m=\u001b[39m suggestions\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mword)\n",
      "File \u001b[0;32m~/linguas-indigenas/nlp-autocomplete/src/accuracy/dijkstra/predictions.py:192\u001b[0m, in \u001b[0;36mget_all_predictions\u001b[0;34m(text_sentence, tokenizer, model, top_k, verbose, device)\u001b[0m\n\u001b[1;32m    190\u001b[0m Node, Token \u001b[39m=\u001b[39m create_node_class(tokenizer, model, text_sentence, device)\n\u001b[1;32m    191\u001b[0m root \u001b[39m=\u001b[39m Node(\u001b[39mNone\u001b[39;00m, Token(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m nxt \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39;49mget_long_neighbor()\n\u001b[1;32m    194\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    195\u001b[0m probs: \u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/linguas-indigenas/nlp-autocomplete/src/accuracy/dijkstra/predictions.py:147\u001b[0m, in \u001b[0;36mcreate_node_class.<locals>.Node.get_long_neighbor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_long_neighbor\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_token_list \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__find_next_token_list__()\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_token_list:\n\u001b[1;32m    150\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/linguas-indigenas/nlp-autocomplete/src/accuracy/dijkstra/predictions.py:115\u001b[0m, in \u001b[0;36mcreate_node_class.<locals>.Node.__find_next_token_list__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m indices\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m ix: \u001b[39m-\u001b[39mtoken_probs[ix])\n\u001b[1;32m    114\u001b[0m \u001b[39m# create all pairs\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m all_pairs \u001b[39m=\u001b[39m [(token_probs[i], Token(i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices]\n\u001b[1;32m    117\u001b[0m all_pairs_filtered \u001b[39m=\u001b[39m []\n\u001b[1;32m    118\u001b[0m found_punctuation \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/linguas-indigenas/nlp-autocomplete/src/accuracy/dijkstra/predictions.py:115\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m indices\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m ix: \u001b[39m-\u001b[39mtoken_probs[ix])\n\u001b[1;32m    114\u001b[0m \u001b[39m# create all pairs\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m all_pairs \u001b[39m=\u001b[39m [(token_probs[i], Token(i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices]\n\u001b[1;32m    117\u001b[0m all_pairs_filtered \u001b[39m=\u001b[39m []\n\u001b[1;32m    118\u001b[0m found_punctuation \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/linguas-indigenas/nlp-autocomplete/src/accuracy/dijkstra/predictions.py:52\u001b[0m, in \u001b[0;36mcreate_node_class.<locals>.Token.__init__\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# decode the id to get the string\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mdecode(\u001b[39mid\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m token \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m##\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m token:\n",
      "File \u001b[0;32m~/linguas-indigenas/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3509\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3506\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3507\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3509\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3510\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3511\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3512\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3513\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3514\u001b[0m )\n",
      "File \u001b[0;32m~/linguas-indigenas/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    928\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    933\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/linguas-indigenas/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:883\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[39m@overload\u001b[39m\n\u001b[1;32m    880\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_ids_to_tokens\u001b[39m(\u001b[39mself\u001b[39m, ids: List[\u001b[39mint\u001b[39m], skip_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    881\u001b[0m     \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m--> 883\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_ids_to_tokens\u001b[39m(\n\u001b[1;32m    884\u001b[0m     \u001b[39mself\u001b[39m, ids: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]], skip_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    885\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    886\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[39m    added tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[39m        `str` or `List[str]`: The decoded token(s).\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ids, \u001b[39mint\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_checkpoint in model_checkpoints:\n",
    "    print(f'Running {model_checkpoint}:')\n",
    "    run_for_model(model_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
