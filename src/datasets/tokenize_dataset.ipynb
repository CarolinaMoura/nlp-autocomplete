{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import Callable\n",
    "from transformers import BertTokenizer\n",
    "from masking_strategies import mask_random_positions, mask_end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_CLS_and_SEP_false(id_tensor, true_false_tensor, tokenizer):\n",
    "    \"\"\"\n",
    "    Receives a tensor of ids and a tensor of masking, with\n",
    "    True and False values. Returns a copy of the true_false_tensor\n",
    "    where every CLS and SEP position is for sure set to False.\n",
    "\n",
    "    Doesn't mutate any input value and it's free of aliasing.\n",
    "    \"\"\"\n",
    "    CLS = tokenizer.convert_tokens_to_ids(\"[CLS]\")\n",
    "    SEP = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "\n",
    "    ans = true_false_tensor.detach().clone()\n",
    "    ans = ans & (id_tensor != CLS) & (id_tensor != SEP)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(masking_strategy, tensor, tokenizer):\n",
    "    \"\"\"\n",
    "    Mutates tensor accordingly to the masking_strategy.\n",
    "    \n",
    "    Args:\n",
    "        masking_strategy: strategy to create the masked\n",
    "                          input_id.\n",
    "        tensor: tensor of tensors, the last representing\n",
    "                the tokenized inputs.\n",
    "        tokenizer: tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    mask_arr = masking_strategy(tensor)\n",
    "    mask_arr = assure_CLS_and_SEP_false(tensor, mask_arr, tokenizer)\n",
    "    rows = shape[0]\n",
    "\n",
    "    MASKED = tokenizer.convert_tokens_to_ids('[MASKED]')\n",
    "\n",
    "    def indices_to_mask(idx):\n",
    "        \"\"\"\n",
    "        Returns a list containing all positions in the\n",
    "        idx-th row of mask_arr that have to be masked.\n",
    "\n",
    "        0 <= idx < rows has to be satisfied.\n",
    "        \"\"\"\n",
    "        to_mask_positions = mask_arr[idx].nonzero()\n",
    "        return torch.flatten(to_mask_positions).tolist()\n",
    "\n",
    "    for i in range(rows):\n",
    "        selection = indices_to_mask(i)\n",
    "        tensor[i, selection] = MASKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels_and_ids(\n",
    "    sentences: list[str], path_to_save: str, masking_strategy: Callable[[torch.Tensor], None], tokenizer\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves the dictionary of input_ids, labels and attention_mask\n",
    "    to memory at the specified path.\n",
    "\n",
    "    Args:\n",
    "        sentences: list of all the sentences to tokenize.\n",
    "        path_to_save: path to save the tokenized inputs.\n",
    "        masking_strategy:\n",
    "        tokenizer:\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # \"labels\" is the answer key\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].detach().clone()\n",
    "\n",
    "    # corrupt \"input_ids\"\n",
    "    mask(masking_strategy, inputs[\"input_ids\"], tokenizer)\n",
    "\n",
    "    # sanity check\n",
    "    assert(not torch.equal(inputs['labels'], inputs['input_ids']))\n",
    "\n",
    "    # finally save the object to memory\n",
    "    torch.save(\n",
    "        {\n",
    "            \"labels\": inputs[\"labels\"],\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        },\n",
    "        path_to_save,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load huge file full of sentences\n",
    "sentences = []\n",
    "\n",
    "with open('../../data/portuguese_sentences.txt', 'r') as f:\n",
    "    for sentence in f:\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of empty sentences\n",
    "sentences = [sent for sent in sentences if sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2149297\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15% of random positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_labels_and_ids(\n",
    "    sentences,\n",
    "    \"../../data/training_tokenized_random_positions.pt\",\n",
    "    mask_random_positions,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to chop several sentences so the model gets used to sentences lacking full meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_part = len(sentences)//3\n",
    "new_sentences = []\n",
    "\n",
    "for sentence in sentences[:third_part]:\n",
    "    words = list(re.finditer(r'\\w+', sentence))\n",
    "\n",
    "    third_part_words = len(words)//3\n",
    "\n",
    "    for ix in [third_part_words, 2*third_part_words, 3*third_part_words]:\n",
    "        real_index = min(ix, len(words))\n",
    "        start = words[real_index].start()\n",
    "        \n",
    "        if start:\n",
    "            new_sentences.append(sentence[:start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_labels_and_ids(\n",
    "    sentences[:third_part]+new_sentences,\n",
    "    \"../../data/training_tokenized_end_positions.pt\",\n",
    "    mask_end_positions,\n",
    "    tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
